反常的实验现象：
1）训练和测试都跑的时候 总共11273mb 只训练6547mb
2）只训练且不使用relation 1819mb
差的好远啊这

实验搜索：

1，默认no relation no drop clip=1
对lr进行寻找
0.1 0.01 0.001 0.0001 0.00001

暂时的结论是0.1完全不能收敛 且loss开始飙升
其他几个选项基本没有差别
所以决定选定0.001作为lr

13-52-46 -> 13-56-37

2，对clip的作用进行考察
因为观察到模型其实会在5.72k step处下降到1.0左右 但是随后却回升了
默认lr=0.001 no drop no relation
clip=0.01 clip=0.1 clip=1 no clip（clip=0）

14-21-46 -> 14-24-12

大致情况是 四个基本上差别不大 整体趋势都是一样的
所以问题不是出在这里 因此默认clip=0 no clip

3,对layer进行考察
layers= 1 2 4 8
15-02-17 -> 15-03-08
效果都不很垃圾 硬要比的话 1层的loss降得更低点

4,发现embedding默认是freeze的 所以关掉这个开关重现跑一次
layers= 1
15-53-01
仍然是没有什么变化


5,去除掉dataset里边on memory对于tiny data的bug后
重新进行实验
relation=False tinydata=100   embedding的freeze也保持更新状态
layers=1 2 4 8
16-30-12 -> 16-33-58
就1layers来说 和之前有了明显的区别 之前loss在5.5和6之间 而这次在3.5和4之间
并且有明显的log形收敛过程
四个参数之间的差别是 1layer的loss约等于3.5 而2 4 8的loss基本相等 约3.8左右
总体而言四个结果仍然不令人满意

6，因为layer=1时收敛较快 且四个参数的学习曲线基本一致 所以设置layer=1作为默认参数
对lr进行搜索

lr= 0.1 0.01 0.001 0.0001
0.0001的效果貌似很好啊
16-57-35 -> 16-59-30
0.1的loss基本飞到天上了
0.01和0.001的效果基本没有什么差别 在3.5左右徘徊
而0.0001的结果很好 波动很小 而且快速收敛 很好的结果！基本上loss到达了0.01左右

尝试对比一下0.00001 1e-5 看看哪个结果更好点 17-07-42
基本上已经确定模型没有问题了
结果发现0.00001的模型不稳定 波动较大 所以最终决定选取0.0001

7，确定了lr=1e-4 于是对layer进行恢复 观察是否稳定
将layer设置到8层之后 收敛曲线开始变得波动
17-14-39
但值得注意的是 这个曲线仍然是比{1e-5,1layer}的曲线要好的

不过慎重起见还是考虑对8layer时进行1e-5的尝试 17-23-03
（观察期间和上边的所有实验进行了对比 发现明显好了很多）

结果应该是比较明显的 1e-4始终是最好的 但是1e-5在应对多层layer时也会有一定的效果

8，lr=1e-4 layers=8 尝试clip
clip=1 17-39-27
clip=0.1 17-41-25
clip基本上不会有什么作用 不过可能会对relation起作用

9，lr=1e-4 no clip layers=8 relation
17-49-02
这个结果貌似还比no relation好一点
综合来看relation 收敛要稍微更快一些 震动幅度也比较小

先把数据解决了吧

10,
train：
save inter node dic
Save Text Vocab
Inter Node Vocab Size = 164
Source Vocab Size = 126890
Target Vocab Size = 43568
valid:
save inter node dic
Inter Node Vocab Size = 164
Source Vocab Size = 22900
Target Vocab Size = 7675
test:
save inter node dic
Inter Node Vocab Size = 164
Source Vocab Size = 18173
Target Vocab Size = 6290

Pretrain Word = 0.39180 for source
Pretrain Word = 0.58823 for target



1, 20-40-29 完整数据实验 太慢了 所以cut掉了

2, 跑的太慢了，所以考虑将数据转成pickle全部读取到memory里加速
之前的超参数下
2,8,4 per gpus
缩小参数后
24,24,16 three gpus
8,8,5 per gpus
且在on memory的作用下 能够快速加载
目前正在22-25-31上运行

这个超参数忘记是多少了

CUDA_VISIBLE_DEVICES=1,2,3 python __main__.py --batch_size=24 --val_batch_size=24 --infer_batch_size=16 --save=True --activation=GELU --hidden=256 --ff_fold=4 --layers=6 --decoder_layers=1 --relation=True --tiny_data=0 --path_embedding_size=128





3,
6a9Oc4i5gH12WPIJZBUX0Fw7zdD8LxA3
scp -r /var/data/pengh/PathAttention/data/csn_python/raw_data jinzhi@CNGrid12@119.90.38.52:/dat01/jinzhi/pengh/PathAttention/data/csn_python/
ssh g0091

4,进行优化后 感觉效果也不是很明显的样子
CUDA_VISIBLE_DEVICES=1,2,3 python __main__.py --batch_size=8 --val_batch_size=8 --infer_batch_size=4 --save=True --activation=GELU --hidden=256 --ff_fold=4 --layers=6 --decoder_layers=1 --relation=True --tiny_data=0 --path_embedding_size=128
relation_23-53-30
15100mb+14144mb+10006mb = 39250mb

CUDA_VISIBLE_DEVICES=0 python __main__.py --batch_size=8 --val_batch_size=8 --infer_batch_size=4 --save=False --activation=GELU --hidden=256 --ff_fold=4 --layers=6 --decoder_layers=1 --relation=False --tiny_data=0 --path_embedding_size=128
naive_00-17-35
4694mb

上下基本差别了八倍的显存


目前遇到的问题是

1，显存占用问题
虽然加上path之后的模型和不加path的模型 参数数量的大小基本没什么差别
（因为差别仅在于一个node节点的embedding lookup table以及对path序列进行解析的gru，而node节点是封闭词表只有164个）

但是在同样batch的设置下 两个模型对显存的占用却差了八倍左右
超参数设置为：
train_batch_size=8 val_batch_size=8 infer_batch_size=4
hidden=256 d_ff=1024 encoder_layer=6 decoder_layer=1 path_embedding_size=128
max_code_length=512 sub_token_length=5 max_path_length=8 max_path_num=512 max_target_len=6
词表大小设置的是：
source vocab size=17488
target vocab size=6290

这样的设置下两种模型的参数数量基本上均为75.0*1e6
而原始模型单卡占用显存4694mb
在同样的参数设置下，path模型在三张卡上占用显存为
15100mb+14144mb+10006mb = 39250mb




3，模型收敛速度问题
现在正在跑的一个path模型 跑了1天多 目前大概到了第11个epoch
train_loss=3.5328  valid_loss=4.6040
precision=0.2463 recall=0.1832 f1=0.2101
觉得这个收敛过程是不是有点太慢了？





scp -r /var/data/pengh/PathAttention/data/csn_python/raw_data jinzhi@CNGrid12@119.90.38.52:/dat01/jinzhi/pengh/PathAttention/data/csn_python/

scp -r /dat01/jinzhi/pengh/PathAttention/trainer   pengh@39.105.32.27:/var/data/pengh/PathAttention/
scp -r /dat01/jinzhi/pengh/PathAttention/catch  /dat01/jinzhi/pengh/PathAttention/data /dat01/jinzhi/pengh/PathAttention/model /dat01/jinzhi/pengh/PathAttention/__main__.py pengh@39.105.32.27:/var/data/pengh/PathAttention/
scp -r /dat01/jinzhi/pengh/PathAttention/data   pengh@39.105.32.27:/var/data/pengh/PathAttention/
scp -r /dat01/jinzhi/pengh/PathAttention/model   pengh@39.105.32.27:/var/data/pengh/PathAttention/
scp -r /dat01/jinzhi/pengh/PathAttention/__main__.py   pengh@39.105.32.27:/var/data/pengh/PathAttention/



python multi_language_parser.py
python multi_language_parser.py --type=test
python multi_language_parser.py --type=valid
\


relation：
lr:
1e-4:
relation_python_2021-03-05-01-14-36
0.2562899130568593 0.1953627495063635 0.22171684675290096
2 epoch
loss 下降到3.62


8e-5
跑了5个epoch
在第一个epoch结束后f1到达最大值0.2976
随后便逐渐降低 应该没有继续提升的迹象了
relation_python_2021-03-08-00-05-52
0.2865534743513771 0.21743465425895878 0.24725445733849097
4 epoch
这个loss逐渐在降低 一直降低到了3.46 挺好的

6e-5
目前只跑了两个epoch
在第一个epoch到达了0.2695
在第二个epoch到达了0.2996
第三轮到了 28.85 开始降低
relation_python_2021-03-08-10-53-43
0.21453379641998396 0.18838341381400167 0.20060999094382045
epoch 2 差劲
loss的确是在下降 到了3.88

4e-5
在第一个epoch到达了0.2520
随后在第二个epoch到达了0.3207
之后便逐渐降低到0.29左右
relation_python_2021-03-07-16-00-55
0.2774867797799554 0.21645715626282966 0.24320168694811753
6 epoch
loss一直在下降 到3.57


2e-5
第一个epoch 22.82
第二个epoch 28.28
第三个epoch 31.77
第四个epoch 31.33
第五个epoch 33.28
随后便开始逐渐降低
重新测：relation_python_2021-03-08-19-56-11_3.pth
0.13284144678803433 0.16191276807882543 0.14594346937337
4 epoch 差劲
反弹又上去了 差劲


vanilla：
lr：
4e-5
在第二个epoch处达到最大值0.2541

8e-5
在第二个epoch到达0.2317
随后基本上都在这个数值附近徘徊


此外 根据code transformer的配置搞了一套参数
学习率 4e-5
首先模型的计算速度比较快
第一个epoch 24.78
第二个epoch 31.06
但是总体而言是比不上上一个模型的
重新测：relation_python_2021-03-09-00-23-17
0.29154496541122216 0.22245899395906238 0.25235914438739865
6 epoch
一直下降到3.48

感觉上给embedding搞得那么大 不大像是有用的 所以还不如就原先的模型
来调试吧


换了统计方法之后 一下子降低到了0.20一下 哎 这也太难了
一夜回到解放前啊
验证集0.17 测试集0.19
这也太难了吧


整理一下 之前每个模型的最后一个参数的结果


今天晚上把之前所有的废结果都整理一下 然后跑新的模型

啥也没分析出来 最终还是不知道该怎么办
好难受啊

那我还是上小模型吧